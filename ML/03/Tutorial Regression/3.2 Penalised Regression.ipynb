{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Penalised regression\n",
    "\n",
    "<img src=\"pictures/LassoRidge.gif\" width = \"500\" style=\"float: right;\"> \n",
    "## Ridge and Lasso Regression\n",
    "\n",
    "Penalised regression fits a multivariate regression model $y=w_0+w_1x_1+...+w_Dx_D$ by minimising the loss\n",
    "$$ F(\\mathbf{w})=\\frac{1}{2}\\sum_{i=1}^N(y_i-\\sum_{j=1}^Dw_jx_{ij}-w_0)^2+\\frac{\\lambda}{2} R(\\mathbf{w})$$\n",
    "where regularisation term $R(\\mathbf{w})$ penalises large weights. \n",
    "\n",
    "**Ridge penalty** minimises sum of squares of the weights: $$R(\\mathbf{w})=\\sum_{j=1}^Dw_j^2$$ \n",
    "**Lasso penalty** minimises sum of absolute values of the weights: $$R(\\mathbf{w})=\\sum_{j=1}^D|w_j|$$  \n",
    "\n",
    "Increasing parameter $\\lambda$ results in solution getting closer to zero, as seen on the right. For Ridge, both weights decrease proportionally, however for Lasso, the smaller weight becomes zero quicker, resulting in a sparse solution.\n",
    "\n",
    "In this notebook we will apply Ridge and Lasso regression to predict Gestational Age (GA) from volumes of 86 brain structures calculated from 164 MRI scans of preterm babies from Developing Human Connectomme Project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data\n",
    "\n",
    "The code bellow loads the dataset with 86 brain volumes and creates feature matrix `X` and target vector `y`. Run the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
    "\n",
    "def CreateFeaturesTargets(filename):\n",
    "    \n",
    "    df = pd.read_csv(filename,header=None)\n",
    "    \n",
    "    # convert from 'DataFrame' to numpy array\n",
    "    data = df.values\n",
    "\n",
    "    # Features are in columns one to end\n",
    "    X = data[:,1:]\n",
    "    \n",
    "    # Scale features\n",
    "    X = StandardScaler().fit_transform(X)\n",
    "\n",
    "    # Labels are in the column zero\n",
    "    y = data[:,0]\n",
    "\n",
    "    # return Features and Labels\n",
    "    return X, y\n",
    "\n",
    "X,y = CreateFeaturesTargets('datasets/GA-brain-volumes-86-features.csv')\n",
    "\n",
    "print('Number of samples is', X.shape[0])\n",
    "print('Number of features is', X.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear regression\n",
    "\n",
    "In the previous notebook we fitted linear regression to the data and calculated RMSE on the whole set and using cross-validation. Run the code bellow to calculate these measures again. Note the functions `RMSE` and `RMSE_CV`, you will need them later. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "def RMSE(model,X,y):\n",
    "    model.fit(X,y)\n",
    "    y_pred = model.predict(X)\n",
    "    rmse = np.sqrt(mean_squared_error(y, y_pred))\n",
    "    print('RMSE:', round(rmse,2))\n",
    "    return rmse\n",
    "\n",
    "def RMSE_CV(model,X,y):\n",
    "    scores = cross_val_score(model,X,y, scoring='neg_mean_squared_error',cv=5)\n",
    "    rmse_cv = np.sqrt(-np.mean(scores))\n",
    "    print('RMSE_CV:', round(rmse_cv,2))\n",
    "    return rmse_cv\n",
    "\n",
    "# choose linear regression model\n",
    "model_linreg = LinearRegression()\n",
    "\n",
    "# calculate RMSE on whole set\n",
    "rmse = RMSE(model_linreg,X,y)\n",
    "\n",
    "# calculate RMSE using cross-validation\n",
    "rmse_cv = RMSE_CV(model_linreg,X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Activity 1:__ Did linear regression overfit the dataset with 86 features? How did you decide on that?\n",
    "\n",
    "__Answer__:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge regression\n",
    "__Activity 2:__ Fill in the code bellow to calculate RMSE and RMSE_CV for `Ridge` regression with default value of `alpha`. Run the code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "# choose linear regression model\n",
    "model = Ridge()\n",
    "print('alpha=', model.alpha)\n",
    "\n",
    "# calculate RMSE on whole set\n",
    "rmse = None\n",
    "\n",
    "# calculate RMSE using cross-validation\n",
    "rmse_cv = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the default value of `alpha`? Did the ridge penalty reduce overfitting?\n",
    "\n",
    "__Answer:__ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lasso regression\n",
    "__Activity 3:__ Fill in the code bellow to calculate RMSE and RMSE_CV for `Lasso` regression with default value of `alpha`. Run the code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "# choose linear regression model\n",
    "model = Lasso()\n",
    "print('alpha=', model.alpha)\n",
    "\n",
    "# calculate RMSE on whole set\n",
    "rmse = None\n",
    "\n",
    "# calculate RMSE using cross-validation\n",
    "rmse_cv = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the default value of `alpha`? Did the lasso penalty reduce overfitting?\n",
    "\n",
    "__Answer:__ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: Compare Ridge and Lasso Regression\n",
    "In this exercise we will tune Ridge and Lasso regression models to predict GA from 86 brain volumes. We will calculate the performance and plot the coefficients of the fitted models.\n",
    "\n",
    "### Task 3.1: Ridge regression\n",
    "Fit `Ridge` regression model. Fill in the code bellow to perform grid search for optimal parameter `alpha` using `GridSearchCV`. Save the tuned Ridge regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "# grid for hyperparameter alpha \n",
    "parameters = {\"alpha\": np.logspace(-3,3,100)}\n",
    "\n",
    "# create ridge model\n",
    "model = Ridge()\n",
    "\n",
    "# perform grid search\n",
    "grid_search = GridSearchCV(None, None,cv=5,scoring = 'neg_mean_squared_error')\n",
    "\n",
    "\n",
    "# remember optimised model\n",
    "model_ridge = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print out the optimal `alpha`, RMSE on training set and the best cross-validated RMSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print optimal alpha\n",
    "print('alpha=', round(None,2))\n",
    "\n",
    "# Calculate RMSE and RMSE_CV\n",
    "rmse = None\n",
    "rmse_cv = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the code bellow to see the evolution of RMSE_CV for Ridge with increasing `alpha`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "\n",
    "plt.plot(np.linspace(-3,3,100),np.sqrt(-grid_search.cv_results_['mean_test_score']))\n",
    "plt.xlabel('log(alpha)')\n",
    "plt.ylabel('RMSE CV')\n",
    "_=plt.title('Cross-validated RMSE for Ridge on logaritmic scale')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3.2: Lasso regression\n",
    "Fit `Lasso` regression model. Perform grid search for optimal parameter `alpha` using `GridSearchCV`. Save the tuned Lasso regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "# grid for hyperparameter alpha\n",
    "parameters = {\"alpha\": np.logspace(-1,3,100)}\n",
    "\n",
    "# create lasso model\n",
    "model = None\n",
    "\n",
    "# perform grid search\n",
    "grid_search = None\n",
    "\n",
    "\n",
    "# remember optimised model\n",
    "model_lasso = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print out the optimal `alpha`, RMSE on training set and the best cross-validated RMSE. Does it perform as well as ridge?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print optimal alpha\n",
    "print('alpha=', round(None,2))\n",
    "\n",
    "# Calculate RMSE and RMSE_CV\n",
    "rmse = None\n",
    "rmse_cv = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Answer:__ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill in the code bellow to see the evolution of RMSE_CV for Lasso with increasing `alpha`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(None,None)\n",
    "plt.xlabel('log(alpha)')\n",
    "plt.ylabel('RMSE CV')\n",
    "plt.title('Cross-validated RMSE for Lasso on logaritmic scale')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3.3: Plot the weights\n",
    "\n",
    "Finally let's plot the coefficients $\\mathbf{w}$ of the fitted `LinearRegression`, `Ridge` and `Lasso`. Fill in the code to plot the weights. See whether the coefficients are smaller for penalised regression and whether they are sparser for Lasso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = [12,3])\n",
    "plt.subplot(121)\n",
    "\n",
    "# Compare weights of linear, Ridge and Lasso regression\n",
    "plt.plot(model_linreg.coef_,'bo',label = 'Linear', markersize = 5)\n",
    "plt.plot(None,'r*',label = 'Ridge')\n",
    "plt.plot(None,'g^',label = 'Lasso', markersize = 5)\n",
    "plt.title('Weights')\n",
    "plt.legend()\n",
    "plt.xlabel('Feature $k$')\n",
    "plt.ylabel('Weight $w_k$')\n",
    "\n",
    "# Compare weights od Ridge and Lasso regression\n",
    "plt.subplot(122)\n",
    "plt.plot(None,'r*',label = 'Ridge')\n",
    "plt.plot(None, model_lasso.coef_,'g^',label = 'Lasso', markersize = 5)\n",
    "plt.title('Weights')\n",
    "plt.xlabel('Feature $k$')\n",
    "plt.ylabel('Weight $w_k$')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
