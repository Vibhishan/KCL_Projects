{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Non-linear regression\n",
    "### Polynomial Ridge regression\n",
    "Non-linear regression can be achieved by combining a non-linear feature transformation $\\boldsymbol{\\phi}(\\mathbf{x})$ with linear regression models. The prediction model will become $y=\\boldsymbol{\\phi}(\\mathbf{x})^T\\mathbf{w}$. An example is a polynomial feature transformation $\\boldsymbol{\\phi}(x)=(1,x,...,x^M)$. \n",
    "\n",
    "The **Non-linear Ridge regression** is obtained by minimising the loss\n",
    "$$ F(\\mathbf{w})=\\frac{1}{2}\\sum_{i=1}^N(y_i-\\boldsymbol{\\phi}(\\mathbf{x}_i)^T\\mathbf{w})+\\frac{\\lambda}{2} \\mathbf{w}^T\\mathbf{w}$$\n",
    "\n",
    "<img src=\"pictures/KRRsigma.gif\" width = \"450\" style=\"float: right;\"> \n",
    "### Kernel Ridge Regression\n",
    "The regression model can be alternatively defined using a **dual representation** $\\mathbf{a}$ \n",
    "$$\\hat{y}=\\sum_{i=1}^N\\kappa(\\mathbf{x},\\mathbf{x}_i)a_i$$\n",
    "where $\\kappa(\\mathbf{x},\\mathbf{x}_i)$ is a kernel measuring simmilarity between samples $\\mathbf{x}$ and $\\mathbf{x}_i$, such as Gaussian Kernel. Predictions using Kernel Ridge regression are evaluated as\n",
    "$$\\hat{y}=\\mathbf{k(x)}^T(\\mathbf{K}+\\lambda\\mathbf{I})^{-1}\\mathbf{y}$$\n",
    "where $\\mathbf{k(x)}=(\\kappa(\\mathbf{x},\\mathbf{x}_1),...,\\kappa(\\mathbf{x},\\mathbf{x}_N))^T$ is a vector of similarities of the training samples with the new sample $\\mathbf{x}$ and \n",
    "$$\\mathbf{K}=\\begin{pmatrix}\\kappa(\\mathbf{x}_1,\\mathbf{x}_1)&\\dots&\\kappa(\\mathbf{x}_1,\\mathbf{x}_N)\\\\\\vdots& \\ddots & \\vdots\\\\ \\kappa(\\mathbf{x}_N,\\mathbf{x}_1)&\\dots&\\kappa(\\mathbf{x}_N,\\mathbf{x}_N) \\end{pmatrix}$$\n",
    "is the matrix of pair-wise similarities between training samples. Kernel Ridge regression model fitted to three datapoints with Gaussian kernel with increasing value of $\\sigma$ is shown on the right.\n",
    "\n",
    "In this notebook we will demonstrate fitting of **Polynomial Ridge** and **Kernel Ridge Regression** to predict the GA from a single feature (volume of cortex) and from volumes of 6 brain structures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data\n",
    "\n",
    "The code bellow imports the essential libraries and loads the dataset with 6 brain volumes and creates feature matrix `X` and target vector `y`. Run the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
    "\n",
    "def CreateFeaturesTargets(filename):\n",
    "    \n",
    "    df = pd.read_csv(filename,header=None)\n",
    "    \n",
    "    # convert from 'DataFrame' to numpy array\n",
    "    data = df.values\n",
    "\n",
    "    # Features are in columns one to end\n",
    "    X = data[:,1:]\n",
    "    \n",
    "    # Scale features\n",
    "    X = StandardScaler().fit_transform(X)\n",
    "\n",
    "    # Labels are in the column zero\n",
    "    y = data[:,0]\n",
    "\n",
    "    # return Features and Labels\n",
    "    return X, y\n",
    "\n",
    "X,y = CreateFeaturesTargets('datasets/GA-brain-volumes-6-features.csv')\n",
    "\n",
    "print('Number of samples is', X.shape[0])\n",
    "print('Number of features is', X.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Univariate non-linear regression\n",
    "\n",
    "We will explore univariate non-linear regression to understand behaviour of Polynomial Ridge and Gaussian Kernel Ridge regression models. We predict age at scan from the volume of cortex, the first feature in six brain tissue dataset. \n",
    "\n",
    "### Create univariate dataset\n",
    "\n",
    "First we extract the cortical volumes. Run the code bellow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract volume of cortex\n",
    "X_cortex = X[:,0].reshape(-1,1)\n",
    "\n",
    "# Print dimensions\n",
    "print('Number of samples is', X_cortex.shape[0])\n",
    "print('Number of features is', X_cortex.shape[1])\n",
    "\n",
    "# Plot the dataset\n",
    "plt.scatter(X_cortex, y) \n",
    "plt.xlabel('Cortical Volume')\n",
    "plt.ylabel('Age at scan (weeks)')\n",
    "plt.title('Univariate dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next cell you are given functions to calculate cross-validated RMSE and plot a univariate regression model. Look at them and run the cell, you will need these functions later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RMSE_CV(model,X,y):\n",
    "    scores = cross_val_score(model, X, y, cv=5, scoring='neg_mean_squared_error')\n",
    "    print('Average cross-validated RMSE: {} weeks '.format(round(np.sqrt(-scores.mean()),2)))\n",
    "\n",
    "def PlotRegressionCurve(model, X, y):\n",
    "    # Plot the data\n",
    "    plt.scatter(X, y) \n",
    "    plt.xlabel('Volume')\n",
    "    plt.ylabel('GA')\n",
    "\n",
    "    # Plot the model\n",
    "    x = np.linspace(X.min(),X.max(),101) \n",
    "    x = x.reshape(-1, 1) \n",
    "    y = model.predict(x) \n",
    "    plt.plot(x, y, 'r-') \n",
    "    plt.ylim([25,46])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Polynomial Ridge Regression\n",
    "This code fits, plots and evaluates the polynomial ridge regression model. Note that: \n",
    "* The model is a `Pipeline` of `PolynomialFeatures`, `StandardScaler` and `Ridge`. \n",
    "* Standard scaler normalises the features after the polynomial transformation and this improves the performance of Ridge regression.\n",
    "* We exclude the feature 1 from `PolynomialFeatures`, because `Ridge` will create an intercept that is not penalised. This also improves the performance of the model.\n",
    "\n",
    "__Activity 1:__ Play with the parameters `degree` and `alpha` to see the effect on the curve and performance. Answer the following questions:\n",
    "* Can you find a setting with the lowest error?\n",
    "* Set polynomial `degree` to 10 and `alpha` to zero. Is the model overfitted?\n",
    "* Find the parameter `alpha` to reduce overfitting for `degree` 10. \n",
    "\n",
    "__Answer:__ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# create model\n",
    "model = Pipeline((\n",
    "(\"poly_features\", PolynomialFeatures(degree=2, include_bias=False)),\n",
    "(\"scaler\", StandardScaler()),\n",
    "(\"ridge\", Ridge(alpha=0)),))\n",
    "\n",
    "# fit model\n",
    "model.fit(X_cortex,y)\n",
    "\n",
    "# evaluate model\n",
    "RMSE_CV(model,X_cortex,y)\n",
    "\n",
    "# plot model\n",
    "PlotRegressionCurve(model, X_cortex, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code bellow performs the grid search to find the best parameters for the Polynomial Ridge Regression model that we defined above. Run it to find the best fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define parameter grid\n",
    "parameters = {\"poly_features__degree\": range(1,15),\n",
    "             \"ridge__alpha\":np.logspace(-3,3,100)}\n",
    "\n",
    "# perform grid search\n",
    "grid_search = GridSearchCV(model, parameters,cv=5)\n",
    "grid_search.fit(X_cortex,y)\n",
    "\n",
    "# calculate best CV RMSE\n",
    "RMSE_CV(grid_search.best_estimator_,X_cortex,y)\n",
    "# print best parameters\n",
    "print('Best degree: ',grid_search.best_estimator_.named_steps[\"poly_features\"].degree)\n",
    "print('Best alpha: ',round(grid_search.best_estimator_.named_steps[\"ridge\"].alpha,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kernel Ridge Regression\n",
    "\n",
    "This code fits, plots and evaluates the kernel ridge regression model `KernelRidge`. Note that: \n",
    "* The kernel has been set to Gaussian using parameter `kernel='rbf'`\n",
    "* Parameter `gamma` represents $\\frac{1}{2\\sigma}$, where $\\sigma$ is the standard deviation of the Gaussian kernel. Note that small values of `gamma` correspond to a large kernel and other way round.\n",
    "* Parameter `alpha` determines strengths of the regularisation.\n",
    "\n",
    "__Activity 2:__ Play with the parameters `degree` and `alpha` to see the effect on the curve and performance. Answer the following questions:\n",
    "* Keep `alpha` fixed to `1e-5` and change values of `gamma` to see the effect of the kernel size on the curve. You can for example try settings `1e-5`, `1e-3`, `1e-1`, `1e1` and `1e3`. Which setting performs the best?\n",
    "* Set `gamma=1` while keeping `alpha=1e-5`. Is the model overfitted?\n",
    "* Find the parameter `alpha` to reduce overfitting for `gamma=1e1`.\n",
    "\n",
    "__Answer:__ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.kernel_ridge import KernelRidge\n",
    "\n",
    "# create model\n",
    "model = KernelRidge(kernel='rbf',gamma=1e0,alpha=1e-5)\n",
    "\n",
    "# fit model\n",
    "model.fit(X_cortex,y)\n",
    "\n",
    "# evaluate model\n",
    "RMSE_CV(model,X_cortex,y)\n",
    "\n",
    "# plot model\n",
    "PlotRegressionCurve(model, X_cortex, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code bellow performs the grid search to find the best parameters for the Kernel Ridge Regression model that we defined above. Run it to find the best fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameter grid\n",
    "parameters = {\"alpha\": np.logspace(-5, 5, 11), \n",
    "              \"gamma\": np.logspace(-5, 5, 11)}\n",
    "\n",
    "# perform grid search\n",
    "grid_search = GridSearchCV(model, parameters,cv=5)\n",
    "grid_search.fit(X_cortex,y)\n",
    "\n",
    "# calculate best CV RMSE\n",
    "RMSE_CV(grid_search.best_estimator_,X_cortex,y)\n",
    "# print best parameters\n",
    "print('Gamma: {} Alpha: {}'.format(round(grid_search.best_estimator_.gamma,3),round(grid_search.best_estimator_.alpha,3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4\n",
    "\n",
    "In this exercise we will fit multivariate non-linear regression model to the dataset with volumes of 6 brain tissues, calculate cross-validater RMSE and check the bias error in the model. We will compare three models\n",
    "* Linear Ridge Regression\n",
    "* Polynomial Ridge Regression\n",
    "* Kernel Ridge Regression with Gaussian Kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4.1: Linear Ridge Regression\n",
    "\n",
    "The code bellow tunes Linear Ridge Regression model to the dataset with 6 features, with feature matrix `X` and target vector `y`. Run the code and note the performance. The tuned model is saved in `model_lin`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Linear Ridge Regression:')\n",
    "\n",
    "# grid for hyperparameter alpha \n",
    "parameters = {\"alpha\": np.logspace(-3,3,7)}\n",
    "\n",
    "# create ridge model\n",
    "model = Ridge()\n",
    "\n",
    "# perform grid search\n",
    "grid_search = GridSearchCV(model, parameters,cv=5)\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "# remember optimised model\n",
    "model_lin = grid_search.best_estimator_\n",
    "\n",
    "# Print optimal alpha\n",
    "print('Best alpha=', round(model_lin.alpha,3))\n",
    "\n",
    "# Calculate RMSE_CV\n",
    "rmse_cv = RMSE_CV(model_lin,X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your task is now to plot expected vs predicted target values to see whether there is the bias in the linear model. Complete the function `PlotTargets` to do that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PlotTargets(model,X,y):\n",
    "    # predict targets\n",
    "    y_pred = None\n",
    "    # plot expected targets on x axis and predicted targets on y axis\n",
    "    plt.plot(None,None,'o', label = 'Target values')\n",
    "    plt.plot([27,45],[27,45],'r', label = '$y=\\hat{y}$')\n",
    "    plt.xlabel('Expected target values')\n",
    "    plt.ylabel('Predicted target values')\n",
    "    \n",
    "PlotTargets(model_lin,X,y)\n",
    "plt.title('Linear Ridge Regression')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Does the plot show bias error?\n",
    "\n",
    "__Answer:__ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4.2: Polynomial Ridge Regression\n",
    "\n",
    "Next, you will tune the polynomial ridge regression, measure its performance and plot the target values to see whether there is still bias.\n",
    "\n",
    "Complete the code before to tune the model. Note that it is saved in `model_poly`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model\n",
    "model = Pipeline((\n",
    "(\"poly_features\", PolynomialFeatures(include_bias=False)),\n",
    "(\"scaler\", StandardScaler()),\n",
    "(\"ridge\", Ridge())))\n",
    "\n",
    "# define parameter grid\n",
    "parameters = {\"poly_features__degree\": range(1,5),\n",
    "             \"ridge__alpha\":np.logspace(-3,3,7)}\n",
    "\n",
    "# perform grid search\n",
    "grid_search = None\n",
    "\n",
    "\n",
    "# remember optimised model\n",
    "model_poly = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complete the code bellow to print the optimal parameters and evaluate performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Polynomial Ridge Regression:')\n",
    "\n",
    "# print optimal parameters\n",
    "print('Best degree: ', model_poly.named_steps[\"poly_features\"].degree)\n",
    "print('Best alpha: ',round(None,3))\n",
    "\n",
    "# Calculate CV RMSE\n",
    "RMSE_CV(model_poly,X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is the performance better than for Linear Ridge?\n",
    "\n",
    "__Answer:__ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the expected vs predicted target values using the function `PlotTargets` that you created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot\n",
    "\n",
    "plt.title('Polynomial Ridge Regression')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is there less bias than for the linear model?\n",
    "\n",
    "__Answer:__ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4.3: Kernel Ridge Regression\n",
    "\n",
    "Finally, you will tune the Gaussian Kernel Ridge Regression, measure its performance and plot the target values to see whether the bias error is further reduced.\n",
    "\n",
    "Complete the code before to tune the model. Note that it is saved in `model_kernel`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model\n",
    "model = None\n",
    "\n",
    "# define parameter grid\n",
    "parameters = {\"alpha\": np.logspace(-5, 5, 11), \n",
    "              \"gamma\": np.logspace(-5, 5, 21)}\n",
    "\n",
    "# perform grid search\n",
    "grid_search = None\n",
    "\n",
    "\n",
    "# remember optimised model\n",
    "model_kernel = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complete the code bellow to print the optimal parameters and evaluate performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Kernel Ridge Regression:')\n",
    "\n",
    "# print optimal parameters\n",
    "print('Best gamma: ', round(None,3))\n",
    "print('Best alpha: ',round(None,5))\n",
    "\n",
    "# Calculate CV RMSE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is the performance better than for Polynomial Ridge?\n",
    "\n",
    "__Answer:__ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the expected vs predicted target values using the function `PlotTargets` that you created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot\n",
    "\n",
    "plt.title('Kernel Ridge Regression')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is there less bias than for the polynomial model?\n",
    "\n",
    "__Answer:__ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
