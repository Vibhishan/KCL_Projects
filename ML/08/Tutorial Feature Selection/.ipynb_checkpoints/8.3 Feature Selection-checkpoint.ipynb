{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature selection\n",
    "\n",
    "In this notebook we will look again at the brain structure volumes of preterm babies. We will see how feature selection can prevent overfitting, improve performance of models and how it can also help in feature interpretation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################\n",
    "## RUN THIS\n",
    "###################################\n",
    "# this code is to suppress warnings\n",
    "import warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "####################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Brain structure volumes\n",
    "\n",
    "We will load again the dataset of 86 brain structure volumes of 164 preterm babies. We will revisit prediction of GA from the volumes. Recall, that Multivariate Linear Regression resulted in overfitting of the data and Lasso and Ridge penalties were successful in reducing the overfitting. \n",
    "\n",
    "### Load data\n",
    "\n",
    "The code below loads the data, creates the feature matrix and the target vector and performs the feature scaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# read spreadsheet using pandas\n",
    "data = pd.read_csv(\"datasets/GA-structure-volumes-preterm.csv\",header=None)\n",
    "# convert from 'DataFrame' to numpy array\n",
    "structure_volumes = data.to_numpy()\n",
    "# Features\n",
    "X = structure_volumes[:,1:]\n",
    "# Targets\n",
    "y = structure_volumes[:,0]\n",
    "# checking the size of the feature and target arrays \n",
    "# note they must agree in the first dimension\n",
    "print('Features shape: {}; Targets shape: {}'.format(X.shape,y.shape)) \n",
    "# we have 86 features and 164 samples\n",
    "\n",
    "# Scale features\n",
    "X = StandardScaler().fit_transform(X)\n",
    "print('Performed feature scaling.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are interested in finding out which features are selected by our models as most predictive of the age at scan. The code below reads the structure names and stores them as a `dataframe` object `structure_names`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read file with structure names\n",
    "structure_names = pd.read_csv('datasets/labels', header = None, sep='\\t')\n",
    "structure_names[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multivariate linear regression \n",
    "\n",
    "Multivariate linear regression overfitted the data. Rembember the performance of linear regression to see whether feature selection can help us improve it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "model = LinearRegression()\n",
    "scores = cross_val_score(model, X, y, scoring = 'neg_mean_squared_error')\n",
    "print('Linear regression: Cross-validated RMSE is ', round(np.sqrt(-scores.mean()),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge\n",
    "\n",
    "Previously, we have found that optimal `alpha` for Ridge regression was around 45. This setting significanlty reduced overfitting. Let's rerun this model. Remember the performance of Ridge regression as a baseline for good performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "model = Ridge(alpha = 45)\n",
    "scores = cross_val_score(model, X, y, scoring = 'neg_mean_squared_error')\n",
    "print('Ridge regression: Cross-validated RMSE is ', round(np.sqrt(-scores.mean()),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now explore different feature selection techniques in Scikit-learn.\n",
    "\n",
    "\n",
    "## Univatiate feature selection\n",
    "\n",
    "### Pearson's correlation coefficient\n",
    "\n",
    "Correlation coefficient can be calaculated using function `pearsonr` from `scipy.stats` module. High correlation (positive and negative) means close to linear relationship with target values. Note that majority, but not all, volumes are highly correlated with GA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "n = X.shape[1]\n",
    "cc = np.zeros(n)\n",
    "for i in range(n):\n",
    "    cc[i]=pearsonr(X[:,i],y)[0]\n",
    "\n",
    "plt.figure(figsize = [16,4])\n",
    "plt.bar(np.arange(n),cc)\n",
    "plt.title('Pearsons correlation coefficient', fontsize = 18)\n",
    "plt.xlabel('Feature', fontsize = 16)\n",
    "plt.ylabel('Correlation coefficient', fontsize = 16)\n",
    "plt.axis([-1,86,0,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F-score\n",
    "\n",
    "Scikit-learn works with F-values rather than Pearson's Correlation Coefficient. However these two are equivalent for feature selection.\n",
    "\n",
    "The F-value can be calculated using `f_regression` in `sklearn`. \n",
    "\n",
    "**Activity 1.1:** Complete the code below to plot F-scores using a `bar` plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import f_regression\n",
    "\n",
    "f_score = f_regression(X,y)[0]\n",
    "\n",
    "# plot f-scores\n",
    "plt.figure(figsize = [16,4])\n",
    "plt.bar(np.arange(n),None)\n",
    "plt.title('F-value', fontsize = 18)\n",
    "plt.xlabel('Feature', fontsize = 16)\n",
    "plt.ylabel('F-value', fontsize = 16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Activity 1.2:** Plot relationship between Person's correlation coefficient and F-score using `plot`. Are they equivalent?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot relationship\n",
    "plt.plot(None,None,'*')\n",
    "plt.xlabel(\"Person's correlation coefficient\", fontsize = 16)\n",
    "plt.ylabel('F-value', fontsize = 16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting features based on F-value\n",
    "<img src=\"pictures/brain.png\" width = \"250\" style=\"float: right;\"> \n",
    "\n",
    "We can select top scoring features to improve performance of the method. Scikit-learn offers two in-build functions to do that: `SelectKBest` and `SelectPercentile`. Let's start by selecting 4 best features using `SelectKBest`.\n",
    "\n",
    "Run the code below to transform the feature matrix `X` into the matrix of selected features `X_selected`. \n",
    "\n",
    "**Activity 1.3:** Check\n",
    "* the size of the new matrix - is it what you expect?\n",
    "* the indices of the features that have been selected.\n",
    "* the names of the selected features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest\n",
    "\n",
    "# define feature selection model\n",
    "k=4\n",
    "selector = SelectKBest(f_regression, k = k)\n",
    "\n",
    "# select features\n",
    "X_selected = selector.fit_transform(X,y)\n",
    "\n",
    "# Shape of the matrix\n",
    "print('Shape of the new matrix: ', X_selected.shape)\n",
    "\n",
    "# Indices of the selected features\n",
    "ind = np.where(selector.get_support())[0]\n",
    "print('Indices: ', ind)\n",
    "\n",
    "# Print the names of the selected structures\n",
    "print('\\n')\n",
    "for i in range(k):\n",
    "    print(structure_names.loc[ind[i],1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Univariate feature selection for improved prediction\n",
    "\n",
    "**Activity 1.4:** Let's now apply the multivariate linear regression to the selected features and see whether performance improves. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select and fit linear regression model to selected features\n",
    "model = None\n",
    "model.fit(None,y)\n",
    "\n",
    "# Calculate and print RMSE\n",
    "scores = cross_val_score(model, X_selected, y, scoring = 'neg_mean_squared_error')\n",
    "print('Linear regression: Cross-validated RMSE is ', round(np.sqrt(-scores.mean()),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that we reduced overfitting but we did not reach performance of Lasso or Ridge. \n",
    "\n",
    "**Activity 5:** Try to vary the number of selected features to see what works best. Remember this performance as the best for univariate feature selection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1\n",
    "\n",
    "Select 4 top scoring features using mutual information. Do you obtain the same or different features as for correlation coefficient?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import mutual_info_regression\n",
    "\n",
    "# set number of features to select\n",
    "k=4\n",
    "\n",
    "# Create feature selector\n",
    "selector = None\n",
    "\n",
    "# select features\n",
    "X_selected = None\n",
    "\n",
    "# Indices of the selected features\n",
    "ind = None\n",
    "\n",
    "# Print the names of the selected structures\n",
    "print('\\n')\n",
    "for i in range(k):\n",
    "    print(structure_names.loc[ind[i],1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model based feature selection\n",
    "\n",
    "### Lasso\n",
    "\n",
    "We will now select the features based on `Lasso` model. We have previously found that setting `alpha=0.16` results in a best Lasso model for our example. Code below creates the model, calculates its performance and prints out the number of sparse coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "model = Lasso(alpha=0.16)\n",
    "scores = cross_val_score(model, X, y, scoring = 'neg_mean_squared_error')\n",
    "print('Lasso regression: Cross-validated RMSE is ', round(np.sqrt(-scores.mean()),2))\n",
    "\n",
    "model.fit(X,y)\n",
    "print('\\n Non-zero coefficients')\n",
    "print(model.sparse_coef_)\n",
    "print('\\n There are {} non-zero coefficients.'.format(model.sparse_coef_.count_nonzero()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below extracts indices of non-zero Lasso coefficients and lists the names of the selected structures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# indices of non-zero elements\n",
    "ind = model.sparse_coef_.nonzero()[1]\n",
    "print('Indices of non-zero elements: ', ind)\n",
    "print('\\n')\n",
    "\n",
    "# print names of selected structures\n",
    "print('Selected structures: \\n')\n",
    "for i in range(ind.size):\n",
    "    print(structure_names.loc[ind[i],1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2\n",
    "\n",
    "We will now use `LassoCV` model for feature selection but fit the selected features using `LinearRegression`. \n",
    "* Implement the feature selection using selector `SelectFromModel` with `LassoCV` model.\n",
    "* Calculate the performance of `LinearRegression` using the selected features.\n",
    "* Experiment with different thresholds for Lasso coeficients to see which one results in best performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "# Create selector with LassoCV model\n",
    "selector = None\n",
    "\n",
    "# Perform feature transformation\n",
    "X_selected = None\n",
    "\n",
    "# Create and fit linear regression model to selected features\n",
    "model = None\n",
    "model.fit(None,y)\n",
    "\n",
    "# Calculate and print RMSE\n",
    "scores = cross_val_score(model, None, y, scoring = 'neg_mean_squared_error')\n",
    "print('Linear regression: Cross-validated RMSE is ', round(np.sqrt(-scores.mean()),2))\n",
    "\n",
    "# List the number and names of the selected features\n",
    "ind = None\n",
    "print('\\nSelected {} features: '.format(ind.size),)\n",
    "for i in range(ind.size):\n",
    "    print(structure_names.loc[ind[i],1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that feature selection using LassoCV with optimised threshold results in performance of Linear regression similar to optimised Ridge. This is better than for univariate feature selection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random forest\n",
    "\n",
    "Random forest is very resilient to overfitting. Let's now fit the random forest regressor to the data and see how it performs. Note, that Random forest allows for non-linear models and it is therefore not surprising that it outperforms Linear Regression Models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    " \n",
    "# Select and fit the model\n",
    "model = RandomForestRegressor(n_estimators=20)\n",
    "\n",
    "# Calculate CV RMSE\n",
    "scores = cross_val_score(model, X, y, scoring = 'neg_mean_squared_error')\n",
    "print('Cross-validated RMSE is ', round(np.sqrt(-scores.mean()),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Activity 2.1:** Feature importances can be access as `model.feature_importances_`. Plot the feature importances using a `bar` plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the model\n",
    "model.fit(X,y)\n",
    "\n",
    "# plot feature importances\n",
    "plt.figure(figsize = [16,4])\n",
    "n = X.shape[1]\n",
    "plt.bar(np.arange(n),None)\n",
    "plt.title('Feature importances', fontsize = 18)\n",
    "plt.xlabel('Features', fontsize = 16)\n",
    "plt.ylabel('importances', fontsize = 16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Activity 2.2:** Use selector `SelectFromModel` to select the features from `RandomForestRegressor(n_estimators=20)`. Choose threshold 0.05 and print the names of the selected features. Are they consistent with the ones selected by Lasso or Correlation Coefficient?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create selector with LassoCV model\n",
    "selector = None\n",
    "\n",
    "# Perform feature transformation\n",
    "X_selected = selector.fit_transform(X, y)\n",
    "\n",
    "# List the number and names of the selected features\n",
    "ind = selector.get_support(indices=True)\n",
    "print('Selected {} features: '.format(ind.size),)\n",
    "for i in range(ind.size):\n",
    "    print(structure_names.loc[ind[i],1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recursive feature elimination\n",
    "\n",
    "Scikit learn offers functions `RFE` and `RFECV` to perform recursive feature elimination. Any model can be used to do that, and this time we will chose `Ridge` regression.  \n",
    "\n",
    "Let's first find 6 best features using `RFE` with `Ridge`. Run the code below to fit `RFE` model and print the names of the selected features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "k=6\n",
    "\n",
    "# create ranking model\n",
    "model = Ridge(alpha=45)\n",
    "\n",
    "# create selector\n",
    "selector = RFE(model, n_features_to_select=k)\n",
    "\n",
    "# fit selector\n",
    "selector.fit(X,y)\n",
    "\n",
    "# Print the indices of the selected features\n",
    "ind = np.where(selector.get_support())[0]\n",
    "print('Indices: ', ind)\n",
    "\n",
    "# Print the names of the selected structures\n",
    "print('\\n')\n",
    "for i in range(k):\n",
    "    print(structure_names.loc[ind[i],1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Activity 3.1:** Transform the features, fit linear regression and calculate CV RMSE to see whether we reduced overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select features\n",
    "X_selected = None\n",
    "\n",
    "# Linear regression\n",
    "model = None\n",
    "scores = cross_val_score(model, X_selected, y,scoring = 'neg_mean_squared_error')\n",
    "print('Linear regression: Cross-validated RMSE is ', round(np.sqrt(-scores.mean()),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3\n",
    "Let's now use method `RFECV` that can also automatically select optimal number of features using cross-validation. Write the code to:\n",
    "* Fit the `RFECV` feature selection with `Ridge(alpha=45)` ranking model\n",
    "* Transform the features and fit the `Ridge(alpha = 45)` model to the selected features\n",
    "* Calculate the CV RMSE\n",
    "* Print indices of selected features\n",
    "* Print number of selected features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFECV\n",
    "\n",
    "# create ranking model\n",
    "model = None\n",
    "\n",
    "# Create selector\n",
    "selector = None\n",
    "\n",
    "# Fit the selector and transform the features\n",
    "X_selected = None\n",
    "\n",
    "# Calculate performace of Ridge with selected features\n",
    "scores = None\n",
    "print('Linear regression: Cross-validated RMSE is ', round(np.sqrt(-scores.mean()),2))\n",
    "\n",
    "# Print indices of the selected features\n",
    "ind = None\n",
    "print('Indices: ', ind)\n",
    "\n",
    "# Print number of selected features\n",
    "print('Number of selected features: ', ind.size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recursive feature elimination using Random Forest\n",
    "\n",
    "**Activity 3.2:** Perform recursive feature elimination using `RFECV` and `RandomForestRegressor(n_estimators=20)`. Be patient, this process might take time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFECV\n",
    "\n",
    "model = RandomForestRegressor(n_estimators=20)\n",
    "selector = None\n",
    "selector.fit(X,y)\n",
    "\n",
    "# Print selected features\n",
    "ind = np.where(selector.get_support())[0]\n",
    "print('Indices: ', ind)\n",
    "\n",
    "print('Number of selected features: ', ind.size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Activity 3.3:** Transform the features and fit the `RandomForestRegressor(n_estimators=20)` to see whether CV RMSE improved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select features\n",
    "X_selected = None\n",
    "\n",
    "# Random Forest with reduced features\n",
    "model = None\n",
    "scores = cross_val_score(model, X_selected, y, scoring = 'neg_mean_squared_error')\n",
    "print('Linear regression: Cross-validated RMSE is ', round(np.sqrt(-scores.mean()),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have seen that feature selection can prevent overfiting and improve performance of the model. We have also seen that Random forest is very resilient against overfitting and our it example did not benefit from feature selection. On contrary, it is a very good tool for selecting features for other methods.\n",
    "\n",
    "We have also seen that selected features varied a lot dependent on the selection method. We therefore need to be careful when interpreting the selected features."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
