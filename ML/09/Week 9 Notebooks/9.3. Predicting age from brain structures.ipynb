{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting age from brain structures\n",
    "<img src=\"images/BrainVolumes.png\" width = \"300\" style=\"float: right;\"> \n",
    "\n",
    "In this notebook we will apply what we learned this week to our example of prediction of age of a baby from the volumes of brain structures. We will learn how to tune the parameters of the network and design a non-linear solution.\n",
    "\n",
    "First, we will load the dataset of 86 brain volumes and convert the feature matrix and target vector to tensors. Run the cell below to load the dataset with 86 structures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only do this if you work on Google Colab\n",
    "# run the cell\n",
    "# then upload file 'GA-brain-volumes-86-features.csv'\n",
    "\n",
    "from google.colab import files\n",
    "files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
    "\n",
    "def CreateFeaturesTargets(filename):\n",
    "    \n",
    "    df = pd.read_csv(filename,header=None)\n",
    "    \n",
    "    # convert from 'DataFrame' to numpy array\n",
    "    data = df.values\n",
    "\n",
    "    # Features are in columns one to end\n",
    "    X = data[:,1:]\n",
    "    \n",
    "    # Scale features\n",
    "    X = StandardScaler().fit_transform(X)\n",
    "\n",
    "    # Labels are in the column zero\n",
    "    y = data[:,0].reshape(-1,1)\n",
    "\n",
    "    # return Features and Labels\n",
    "    return X, y\n",
    "\n",
    "X,y = CreateFeaturesTargets('GA-brain-volumes-86-features.csv')\n",
    "\n",
    "# perform scaling of the target values to support better convergence\n",
    "target_scaler = StandardScaler()\n",
    "y = target_scaler.fit_transform(y)\n",
    "\n",
    "print('Number of samples is', X.shape[0])\n",
    "print('Number of features is', X.shape[1])\n",
    "\n",
    "# convert to tensors\n",
    "X = torch.from_numpy(X).float()\n",
    "y = torch.from_numpy(y).float()\n",
    "print('X shape: ', X.shape)\n",
    "print('y shape: ', y.shape)\n",
    "print('X type: ', X.type())\n",
    "print('y type: ', y.type())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note 1:** We have converted the target values to a two dimensional vector and both feature matrix and target vector are Pytorch tensors of type `float`. This is required by Pytorch.\n",
    "\n",
    "**Note 2:** Unlike before we performed scaling of the target values as well. This improves convergence of stochastic gradient descent (regression techniques we used before used analytical solutions or different optimisers)\n",
    "\n",
    "Below is the function `PlotTargets` that we used before to display true and predicted target values. Take a note of the function and run the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def PlotTargets(y_pred,y, label = 'Target values', plot_line=True):\n",
    "    if plot_line:\n",
    "        plt.plot([-3,3],[-3,3],'r', label = '$y=\\hat{y}$')\n",
    "    plt.plot(y,y_pred,'o', label = label)\n",
    "\n",
    "    plt.xlabel('Expected target values')\n",
    "    plt.ylabel('Predicted target values')\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4\n",
    "\n",
    "In this exercise you will train and evaluate a single layer perceptron to predict age of a baby from volumes of 86 brain structures. First we will split the dataset into training, validation and test set. \n",
    "\n",
    "This is different from what we have done before, but cross-validation is rarely used in deep learning, due to long training times. You will see later in this exercise how these three sets are used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# extract test set\n",
    "groups = np.round(y/3)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=42, stratify=groups)\n",
    "\n",
    "# extract validation set \n",
    "groups_val = np.round(y_train/3)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.15, random_state=42, stratify=groups_val)\n",
    "\n",
    "# display info\n",
    "print('Training samples: ', y_train.shape[0])\n",
    "print('Validation samples: ', y_val.shape[0])\n",
    "print('Test samples: ', y_test.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To simplify the code, you are given three functions below:\n",
    "* `train` will perform one training epoch and return the current loss value\n",
    "* `validate` will return the loss value without performing any training.\n",
    "* `RMSE` will calculate the root mean squared error for the trained network and dataset that you specify. It will account for the scaling of the target values as well. Result is in **weeks GA**.\n",
    "\n",
    "Look the at functions and run the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# performs one training epoch\n",
    "# returns MSE loss\n",
    "def train(net,X,y):\n",
    "    # 1. Clear gradients \n",
    "    optimizer.zero_grad() \n",
    "    # 2. Forward pass\n",
    "    prediction = net(X) \n",
    "    # 3. Compute loss\n",
    "    loss = loss_function(prediction, y) \n",
    "    # 4. Calculate gradients\n",
    "    loss.backward()       \n",
    "    # 5. Update network parameters\n",
    "    optimizer.step() \n",
    "    # return MSE loss\n",
    "    return loss.data # we want only value, not gradients\n",
    "\n",
    "# calculates and returns the loss any training\n",
    "def validate(net,X,y):\n",
    "    with torch.no_grad(): # no need to calculate gradients\n",
    "        # Forward pass\n",
    "        prediction = net(X)\n",
    "        # Calculate loss\n",
    "        loss = loss_function(prediction, y)\n",
    "        # return MSE loss\n",
    "        return loss\n",
    "\n",
    "# Calculates RMSE in weeks GA\n",
    "def RMSE(net,X,y):\n",
    "    loss = validate(net,X,y).numpy()\n",
    "    rmse = np.sqrt(loss*target_scaler.var_[0])\n",
    "    return np.round(rmse,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the code that we used to fit linear regression model to the predict brain volume from age in Notebook 9.1. The code will not work for our dataset, you need to modify it.\n",
    "\n",
    "**Task 4.1:** Adjust **architecture** of the network so that it can be used to predict age from 86 structures.\n",
    "\n",
    "**Task 4.2:** You will see that the network does not train properly. First thing we notice that the training loss is increasing. This may be a sign that the **learning rate** is too high. Test smaller learning rates to see whether this will solve the problem. Choose the highest learning rate that is still small enough so that training loss does not increase.\n",
    "\n",
    "**Task 4.3:** Once you tuned your learning rate, you will probably find that the training loss is still steeply decreasing and the network has not yet converged to a good solution. Increase the number of **epochs** to 1000 and see what happens. Looking that the MSE loss plot, how many epochs did you need for the network to converge?\n",
    "\n",
    "**Task 4.4:** You may notice that number of epoch we have is rather arbitrary, and we do not know whether it is too few, and network did not converge yet, or too many, and the network started overfitting. You may also wonder why do we have the **validation set**. We will use it to monitor performance of the network during training. In this task, you will implement monitoring of the training during epochs using validation set as follows:\n",
    "* Create a variable `val_losses` where you will save the validation loss at each epoch. Initialise it before `for` loop similarly to `train_losses`. \n",
    "* At each epoch call function `validate` to calculate loss on the validation set `X_val`, `y_val`. Append the validation loss returned by the function `validate` to the variable `val_losses`.\n",
    "* In the subplot `133` plot validation loss in addition to the training loss.\n",
    "* If needed, change the number of epoch to 10000 to find out when the validation loss starts increasing. \n",
    "\n",
    "**Task 4.5:** We would of course like to choose the model that performs best on the validation set as our final trained model. We therefore need to keep training, while the loss on validation set is decreasing. Once it starts increasing, we will stop training to prevent overfitting. This technique is called **early stopping** and in fact acts as regularisation. To implement early stopping we need to `break` the `for` loop once the validation loss starts increasing. To do that, add this code at the end of the `for` loop:\n",
    "\n",
    "`if(i>1):\n",
    "     if val_losses[i-1]>val_losses[i-2]:\n",
    "         print('Final iteration: ', i)\n",
    "         break`\n",
    "\n",
    "**Note:** Not all runs of the network will perform equally well. This is because we are using gradient descent and the weights of the network are initialised to random values. Therefore the fit will not always converge to an optimal solution. But you will also see that some runs produce a good solution, similar to the penalised regression techniques that we have covered earlier in this module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "class ANRegressor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ANRegressor, self).__init__()\n",
    "        self.layer = nn.Linear(1, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer(x)      \n",
    "        return x\n",
    "\n",
    "# create network\n",
    "net = ANRegressor()\n",
    "print(net)\n",
    "\n",
    "# mean squared error loss\n",
    "loss_function = nn.MSELoss()\n",
    "\n",
    "# stochastic gradient descent optimiser\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=0.2)\n",
    "\n",
    "# train\n",
    "train_losses=[]\n",
    "for i in range(10):\n",
    "    loss = train(net, X_train, y_train)\n",
    "    train_losses.append(loss) # we save losses to display them at the end\n",
    "\n",
    "# calculate training and test performance \n",
    "rmse_train = RMSE(net,X_train,y_train)\n",
    "print('Training RMSE: ', rmse_train)\n",
    "rmse_val = RMSE(net,X_val,y_val)\n",
    "print('Validation RMSE: ', rmse_val)\n",
    "rmse_test = RMSE(net,X_test,y_test)\n",
    "print('Test RMSE: ', rmse_test)\n",
    "\n",
    "# display results\n",
    "plt.figure(figsize=(14,4))\n",
    "\n",
    "# plot training set predictions\n",
    "plt.subplot(131)\n",
    "PlotTargets(net(X_train).data,y_train)\n",
    "plt.title('Training set')\n",
    "\n",
    "# plot validation and test set predictions\n",
    "plt.subplot(132)\n",
    "PlotTargets(net(X_val).data, y_val, label = 'val targets')\n",
    "PlotTargets(net(X_test).data,y_test, label = 'test targets', plot_line=False)\n",
    "plt.title('Validation and Test set')\n",
    "\n",
    "# plot training and validation loss\n",
    "plt.subplot(133)\n",
    "plt.plot(train_losses,label='training loss')\n",
    "plt.title('MSE loss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5 (optional)\n",
    "\n",
    "Do this exercise if you finished early and have time to play with a more complex neural network. We will now tune a multi-layer perceptron to predict age from 6 brain volumes. \n",
    "\n",
    "First, we will load the dataset with 6 brain structures. Note that the code below will overwrite the previous dataset.\n",
    "\n",
    "**Task 5.1:** Fill in the missing code to convert the feature matrix and target vector from numpy arrays to a format suitable for training in Pytorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only do this if you work on Google Colab\n",
    "# run the cell\n",
    "# then upload file 'GA-brain-volumes-6-features.csv'\n",
    "\n",
    "from google.colab import files\n",
    "files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y = CreateFeaturesTargets('GA-brain-volumes-6-features.csv')\n",
    "\n",
    "# perform scaling of the target values to support better convergence\n",
    "target_scaler = StandardScaler()\n",
    "y = target_scaler.fit_transform(y)\n",
    "\n",
    "print('Number of samples is', X.shape[0])\n",
    "print('Number of features is', X.shape[1])\n",
    "\n",
    "# convert to tensors\n",
    "X = None\n",
    "y = None\n",
    "print('X shape: ', X.shape)\n",
    "print('y shape: ', y.shape)\n",
    "print('X type: ', X.type())\n",
    "print('y type: ', y.type())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 5.2:** Create training, validation and test set similarly to exercise 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract test set\n",
    "\n",
    "\n",
    "# extract validation set \n",
    "\n",
    "\n",
    "# display info\n",
    "print('Training samples: ', y_train.shape[0])\n",
    "print('Validation samples: ', y_val.shape[0])\n",
    "print('Test samples: ', y_test.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 5.3:** Perform training and evaluation of the performance using the same code as you developed in Exercise 4. You need to adapt the architecture to take 6 input features, but other than that the code should work. Adjust the learning rate to achieve optimal performance.\n",
    "\n",
    "**Task 5.4:** Implement a multi-layer perceptron architecture as follows:\n",
    "* First `Linear` layer with 6 outputs\n",
    "* `ReLU` activation\n",
    "* Second `Linear` layer with 6 inputs and 1 output\n",
    "\n",
    "See whether you can achieve better performance with this non-linear network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
