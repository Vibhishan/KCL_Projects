{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9.2 Multi-output and multi-layer models\n",
    "\n",
    "In the previous notebook we have seen a simple neural network regressor (Linear regression) and classifier (Logistic regression) that were implemented as a single artificial neuron.\n",
    "\n",
    "In this notebook we will learn how to build more complex network architectures. We will implement a\n",
    "* __multi-output single layer perceptron__ to obtain a multi-label linear classification model\n",
    "* __multi-layer perceptron__ to obtain a non-linear classification model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-label linear classification\n",
    "<img src=\"images/softmax_cropped.png\" width = \"300\" style=\"float: right;\">\n",
    "\n",
    "A single layer perceptron consists of multiple neurons organised in one layer. These  neurons  share  the  same  input  features,  but  each  of  them  produces a different output. The outputs of the linear layer for each neuron $z_k=\\sum_jw_{jk}x_j$ are either passed through the same activation function $f(z)$, or in case of multi-label classification through a shared **softmax** activation function:\n",
    "$$ \\hat{p}_k=\\frac{e^z_k}{\\sum_{j=1}^Ke^z_{j}}$$\n",
    "\n",
    "The linear multi-label classification is implemented with a **single linear layer**, with\n",
    "* the number of **inputs** equal to the number of **features**\n",
    "* the number of **outputs** equal to the number of **classes**\n",
    "\n",
    "For example, if we would like to predict no, mild moderate and severe heart failure from EF and GLS, we need two inputs and three outputs, as implemented in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "l = nn.Linear(2,3)\n",
    "print(l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss will be set to **cross-entropy** using the in-built function `CrossEntropyLoss`. This function combines softmax with cross-entropy loss, so we will not need to implement the activation function in our network. For numerical reasons, Pytorch implements log-softmax followed by negative log-likelihood loss in this function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Multi-label linear classifier\n",
    "\n",
    "In this exercise we will implement a multi-label classifier in Pytorch, to predict no, mild moderate and severe heart failure from EF and GLS. Code below loads and plots the data, and converts the data into Pytorch tensors.\n",
    "\n",
    "Note, that the input features are required to be of type `float`, while output labels need to be of type `long`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only do this if you work on Google Colab\n",
    "# run the cell\n",
    "# then upload file 'heart_failure_data_complete.csv'\n",
    "\n",
    "from google.colab import files\n",
    "files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "df = pd.read_csv('heart_failure_data_complete.csv')\n",
    "data = df.to_numpy()\n",
    "X = data[:,[1,2]]\n",
    "X = StandardScaler().fit_transform(X)\n",
    "y = data[:,0]\n",
    "\n",
    "def PlotData(X,y,fontsize=12):\n",
    "    plt.plot(X[y==0,0],X[y==0,1],'bo',alpha=0.75,markeredgecolor='k',label = 'Healthy')\n",
    "    plt.plot(X[y==1,0],X[y==1,1],'rd',alpha=0.75,markeredgecolor='k',label = 'moderate HF')\n",
    "    plt.plot(X[y==2,0],X[y==2,1],'g^',alpha=0.75,markeredgecolor='k',label = 'severe HF')\n",
    "    plt.title('Diagnosis of Heart Failure', fontsize = fontsize+2)\n",
    "    plt.xlabel('EF', fontsize = fontsize)\n",
    "    plt.ylabel('GLS', fontsize = fontsize)\n",
    "    plt.legend(fontsize = fontsize-2)\n",
    "\n",
    "PlotData(X,y)\n",
    "\n",
    "# convert numpy array to tensor in shape of input size\n",
    "X = torch.from_numpy(X).float()\n",
    "y = torch.from_numpy(y).long()\n",
    "print('X: ', X.shape)\n",
    "print('y: ', y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the function to plot the classification result. Run the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PlotClassification(net,X,y,fontsize=12):\n",
    "\n",
    "    # Create an 1D array of samples for each feature\n",
    "    x1 = np.linspace(-2.5, 2, 1000) \n",
    "    x2 = np.linspace(-3, 3.5, 1000).T # note the transpose\n",
    "    # Creates 2D arrays that hold the coordinates in 2D feature space\n",
    "    x1, x2 = np.meshgrid(x1, x2) \n",
    "    # Flatten x1 and x2 to 1D vector and concatenate into a feature matrix\n",
    "    Feature_space = np.c_[x1.ravel(), x2.ravel()] \n",
    "    \n",
    "    # NEW: convert numpy to torch\n",
    "    Feature_space = torch.from_numpy(Feature_space).float()\n",
    "    # NEW: Predict output scores for the whole feature space    \n",
    "    output_scores = net(Feature_space)\n",
    "    # NEW: Take maximum to get the labels\n",
    "    _,y_pred=torch.max(output_scores, 1)\n",
    "    # NEW: Convert to numpy\n",
    "    y_pred = y_pred.numpy()\n",
    "    \n",
    "    # Resahpe to 2D\n",
    "    y_pred = y_pred.reshape(x1.shape)\n",
    "    # Plot using contourf\n",
    "    plt.contourf(x1, x2, y_pred, cmap = 'summer')\n",
    "    \n",
    "    # Plot data\n",
    "    PlotData(X,y,fontsize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train test split\n",
    "**Task 2.1:** First, split the data into training set and test set. For this, we will use scikit-learn `train_test_split`. Note that this function works on Pytorch tensors the same way as on numpy arrays. Use 33% of the data for testing. Note the types of the split dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = None\n",
    "\n",
    "print('Test features type:', X_test.type())\n",
    "print('Test labels type:', y_test.type())\n",
    "print('Test labels:', y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create and train the network\n",
    "**Task 2.2:** Fill in the code below to create and train multi-label classification model in Pytorch. Make sure that the network is trained using only training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# network architecture\n",
    "class MultiLabelClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MultiLabelClassifier, self).__init__()\n",
    "        self.layer = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = None \n",
    "        return x\n",
    "\n",
    "# create model    \n",
    "net = MultiLabelClassifier()\n",
    "\n",
    "# loss\n",
    "loss_function = None\n",
    "\n",
    "# optimiser\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=0.2)\n",
    "\n",
    "# train for 500 epochs\n",
    "epochs = 500\n",
    "for i in range(epochs):\n",
    "    optimizer.zero_grad() \n",
    "    prediction = None \n",
    "    loss = loss_function(prediction, None) \n",
    "    loss.backward()       \n",
    "    optimizer.step() \n",
    "\n",
    "# Plot result\n",
    "PlotClassification(net,None,None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate training accuracy\n",
    "\n",
    "We will now show how we can predict the labels on the test set using this network. Because softmax and cross-entropy loss are combined, for each sample the network will return three outputs of the linear layer $z_0,z_1,z_2$ that correspond to the three classes. These outputs are referred to as **logits**. Let's test that for an individual feature vector $x=(0,0)$ that we create:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a feature vector of correct shape and type\n",
    "x = torch.tensor((0,0)).reshape(1,2).float()\n",
    "# predict using forward pass\n",
    "z = net(x)\n",
    "# print logits\n",
    "print('Logits: ', z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find the label for this datapoint, we need to find which class returned the largest logit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = torch.argmax(z, dim=1)\n",
    "print('Predicted label: ', y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 2.3:** Fill in the code to predict the labels for the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# forward pass\n",
    "pred=None\n",
    "# find maximum\n",
    "y_pred_train = None\n",
    "print(y_pred_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pytorch does not offer functions for calculating performance measures, by the can use `accuracy_score` from scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "print('Training accuracy: ', accuracy_score(y_train, y_pred_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate on test set\n",
    "\n",
    "**Task 2.3:** To evaluate accuracy on the test set, implement the following:\n",
    "* predict the logits for the test set by running a forward pass through the network\n",
    "* convert logits to label\n",
    "* calculate test accuracy score\n",
    "* plot the classification result for the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# forward pass\n",
    "pred=None\n",
    "\n",
    "# find maximum\n",
    "y_pred_test = None\n",
    "\n",
    "# calculate accuracy\n",
    "print('Test accuracy: ', None)\n",
    "\n",
    "# plot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/MLP_binary_classifier_cropped.png\" width = \"350\" style=\"float: right;\">\n",
    "\n",
    "## Multi-layer perceptron for non-linear classification\n",
    "\n",
    "The flexibility of the neural networks comes from combining many artificial neurons into a single machine learning model organized in multiple layers. Linear layers of neurons with multiple outputs can be stacked one after another resulting in a **multi layer perceptron** model. If we add non-linear activation functions between the layers, we will be able to create flexible non-linear models. Networks consisting of several linear layers are also called **fully connected deep neural networks**. \n",
    "\n",
    "The image on the right illustrates a two-layer network that we will create to build a **non-linear binary classifier**. It has two input features. The first linear layer has two inputs and six outputs. Multiple outputs will give us flexibility to fit a highly non-linear decision boundary. The six outputs of the first layer will become inputs of the second layer, which has only one output. After the first layer, we will have **ReLU** activation, to introduce non-linearity to the network. After the second layer we have **Sigmoid** activation to build a binary classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: Non-linear classifier\n",
    "\n",
    "In this exercise we are going to fit a multi-layer perceptron to a simulated dataset with two co-centric circles. Run the cell below to create and display the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_circles\n",
    "\n",
    "X,y = make_circles(n_samples=500,factor=0.5, noise=0.08)\n",
    "\n",
    "X = torch.from_numpy(X).float()\n",
    "y = torch.from_numpy(y.reshape(-1,1)).float()\n",
    "print('X: ', X.shape)\n",
    "print('y: ', y.shape)\n",
    "\n",
    "def PlotData2(X,y):\n",
    "    y=y.flatten()\n",
    "    plt.plot(X[y==0,0],X[y==0,1],'bo',alpha=0.75,markeredgecolor='k')\n",
    "    plt.plot(X[y==1,0],X[y==1,1],'rd',alpha=0.75,markeredgecolor='k')\n",
    "    plt.axis('equal')\n",
    "    plt.title('Circles', fontsize = 14)\n",
    "    #plt.axis('off')\n",
    "\n",
    "plt.figure(figsize = (5,5))\n",
    "PlotData2(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need a slightly different function to plot the classification result, run the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PlotClassification2(net,X,y):\n",
    "\n",
    "    # Create an 1D array of samples for each feature\n",
    "    a = 0.2\n",
    "    x1 = np.linspace(X[:,0].min()-a, X[:,0].max()+a, 1000) \n",
    "    x2 = np.linspace(X[:,1].min()-a, X[:,1].max()+a, 1000).T # note the transpose\n",
    "    # Creates 2D arrays that hold the coordinates in 2D feature space\n",
    "    x1, x2 = np.meshgrid(x1, x2) \n",
    "    # Flatten x1 and x2 to 1D vector and concatenate into a feature matrix\n",
    "    Feature_space = np.c_[x1.ravel(), x2.ravel()] \n",
    "    \n",
    "    # NEW: convert numpy to torch\n",
    "    Feature_space = torch.from_numpy(Feature_space).float()\n",
    "    # NEW: Predict output scores for the whole feature space    \n",
    "    output_scores = net(Feature_space)\n",
    "    # NEW: Threshold output scores\n",
    "    y_pred = (output_scores>0.5).long()\n",
    "    \n",
    "    # Resahpe to 2D\n",
    "    y_pred = y_pred.reshape(x1.shape)\n",
    "    # Plot using contourf\n",
    "    plt.contourf(x1, x2, y_pred, cmap = 'summer')\n",
    "    \n",
    "    # Plot data\n",
    "    PlotData2(X,y)\n",
    "    plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 3.1:** In the cell below is a working code to fit a single-layer perceptron to the dataset. Run the code. What do you observe? Is the classifier suitable for the dataset?\n",
    "\n",
    "**Answer:** \n",
    "\n",
    "**Task 3.2:** Modify the network architecture so that it can fit this non-linear dataset. In function `__init__` implement \n",
    "* linear `layer1` with two inputs and 6 outputs\n",
    "* `ReLU` activation\n",
    "* linear `layer2` with 6 inputs and one output\n",
    "* `Sigmoid` activation\n",
    "Then modify the `forward` function accordingly. Run the cell to train and display the multi-layer perceptron classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLPClassifier, self).__init__()\n",
    "        self.layer1 = nn.Linear(2, 1)\n",
    "        #self.relu = nn.ReLU() \n",
    "        #self.layer2 = None\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x\n",
    "    \n",
    "def train(net, X, y):\n",
    "    loss_function = nn.BCELoss()\n",
    "    optimizer = torch.optim.SGD(net.parameters(), lr=0.2, momentum=0.75)\n",
    "    epochs = 500\n",
    "    for i in range(epochs):\n",
    "        optimizer.zero_grad() \n",
    "        prediction = net(X) \n",
    "        loss = loss_function(prediction, y) \n",
    "        loss.backward()       \n",
    "        optimizer.step()   \n",
    "    return net\n",
    "\n",
    "net2 = MLPClassifier()\n",
    "train(net2,X,y)\n",
    "PlotClassification2(net2,X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 3.3 (optional):** Simulate an additional dataset to create an independent test set. Calculate accuracy on this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simulate test set\n",
    "X_test, y_test = make_circles(n_samples=500,factor=0.5, noise=0.08)\n",
    "\n",
    "# convert to tensors\n",
    "X_test =None\n",
    "y_test = None\n",
    "print('X_test: ', X_test.shape)\n",
    "print('y_test: ', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict probabilities for class 1\n",
    "pred = None\n",
    "\n",
    "# threshold to create labels\n",
    "y_pred_test = None\n",
    "\n",
    "# calculate accuracy\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
