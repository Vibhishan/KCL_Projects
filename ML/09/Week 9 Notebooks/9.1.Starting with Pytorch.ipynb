{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Starting with Pytorch\n",
    "\n",
    "This is the first of the series of Pytorch tutorials that you will see over last two week of the Machine Learning module. This tutorial covers installation, Pytorch tensors, Autograd and your first neural network based on a single artificial neuron."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up your Pytorch environment\n",
    "\n",
    "Pytorch is a popular environment for implementing and training deep neural network models. You can install Pytorch on your own computer, or you can run your notebooks on Google Colab instead. This week's tutorial will run fine on your laptop, but next week we will need GPU, and Google Colab will be therefore needed.\n",
    "\n",
    "### Install Pytorch locally\n",
    "You can install Pytorch locally by visiting \n",
    "\n",
    "https://pytorch.org/get-started/locally/\n",
    "\n",
    "Run the command from this website for example in your Anaconda Prompt. You do not need a GPU to run the Pytorch on your own computer. By default, the Pytorch runs on CPU. Examples in this notebook are simple and do not require GPU. \n",
    "\n",
    "Next week you will need also GPU, and for that you need to have NVIDIA GPU card and install CUDA. This can be tricky, so we do not provide instructions and refer you to Colab instead.\n",
    "\n",
    "### Use Google Colab\n",
    "To run your notebooks on Google Colab instead, follow these instructions:\n",
    "1. Go to https://colab.research.google.com/\n",
    "2. Click __Upload__\n",
    "3. Upload this notebook\n",
    "\n",
    "You are now ready to start working in Colab. Next week we will also show how to use GPU in Colab.\n",
    "\n",
    "### Import Pytorch\n",
    "\n",
    "Check that Pytorch is available in your environment by importing it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch tensors\n",
    "Pytorch tensors are similar to `numpy` arrays, but have some additional features, including the ability to run on GPU and features to enable automatic differentiation. \n",
    "\n",
    "We can create a random Pytorch tensor by"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor = torch.rand(2, 2, 2)\n",
    "print(tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or by setting predefined values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor2 = torch.tensor([1,2])\n",
    "print(tensor2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can access individual elements of tensors and their shape similarly to numpy arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tensor[0,0,0])\n",
    "print(tensor.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can reshape the tensors using `view` without copying the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tensor.view(2,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can easily convert between numpy arrays and tensors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "a = np.array([1,2,3])\n",
    "t = torch.from_numpy(a)\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a2 = t.numpy()\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Activity 1:** Practise  working  with  Pytorch  tensors.   First,  create  and  display  a Pytorch tensor:\n",
    "* Create a numpy array with values `[1,...,12]` (*Hint:* You can use `np.linspace`). \n",
    "* Convert it to a pytorch tensor.\n",
    "* Print out the tensor as a matrix of size $3\\times 4$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch \n",
    "\n",
    "# Create a numpy array\n",
    "x = None\n",
    "\n",
    "# Convert to tensor\n",
    "t = None\n",
    "\n",
    "# Display in shape 3x4\n",
    "print(None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Activity 2:** Learn how to concatenate two Pytorch tensors\n",
    "* Create two random Pytorch tensors with sizes $1 \\times 2 \\times 4 \\times 4$ and $1 \\times 5 \\times 4 \\times 4$ \n",
    "* Concatenate them on the 1st axis using `torch.cat`(*Hint:* axis are numbered from 0)\n",
    "* Print the dimensions of the concatenated tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create two random torch arrays\n",
    "a = None  \n",
    "b = None  \n",
    "\n",
    "# Concatenate\n",
    "c = None \n",
    "\n",
    "# Print shape\n",
    "print('c.shape: ', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autograd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pytorch offers automatic differentiation to enable training of neural networks. If the tensor stores parameters that we want to learn, we can set the `tensor` atrribute `requires_grad` to `True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = torch.tensor([1.0,2.0], requires_grad=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's minimise the sum of squared error loss with respect to `w` implemented as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s=w**2\n",
    "loss=s.sum() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that Pytorch created functions to calculate the derivatives of each tensor with respect to `w`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(s)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The derivatives are calculated by chain rule, or in other words by **backpropagation** from the variable `loss` trhough internmediate steps (variable `s`) towards the parameters `w`. This is implemented using function `backward`. The gradients can be accessed through `w.grad`.\n",
    "\n",
    "*Note: if you run `backward` twice, Pytorch will complain, re-run the cells above to fix this*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()\n",
    "print(w.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Activity 3:** Practise using the Autograd feature of Pytorch:\n",
    "* Create a Pytorch tensor `y` with values `[0,1]`\n",
    "* Create another Pytorch tensor `p` with values `[0.5,0.5]`. Set `requires_gradient` to `True`.\n",
    "* Implement cross-entropy loss $L=-y_0\\log(p_0)-y_1\\log(p_1)$\n",
    "* Print out the loss value\n",
    "* Calculate the gradients of the loss with respect to `p` and print them out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tensor y = [0,1]\n",
    "y = None\n",
    "# Create tensor p=[0.5.0.5], requires grad\n",
    "p = None\n",
    "\n",
    "# Calculate cross-entropy loss\n",
    "#m = -y*torch.log(p)\n",
    "ce_loss = None\n",
    "# Print loss value\n",
    "print(ce_loss)\n",
    "\n",
    "# Calculate gradients of loss w.r.t. p\n",
    "\n",
    "# Print gradients w.r.t. p\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The first neural network in Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/SLP_cropped.png\" width = \"400\" style=\"float: right;\"> \n",
    "The simplest neural network consists of a single artificial neuron. It can be expressed by equation\n",
    "$$z=\\sum_{j=0}^Dw_jx_j$$\n",
    "$$\\hat{y}=f(z)$$\n",
    "where $x_j$ are input features, $\\hat{y}$ are outputs, $w_j$ are learnable weights and $f$ is an activation function. If we choose **mean squared error** as the loss to be minimised and **identity** as an activation function, we will obtain a simple multivariate **linear regression** with $D$ input features $\\hat{y}=\\sum_{j=0}^Dw_jx_j$.\n",
    "\n",
    "In Pytorch the equation $z=\\sum_{j=0}^Dw_jx_j$ is implemented as a **linear layer** with $D$ inputs and $1$ output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "D=3\n",
    "nn.Linear(D,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Activity 4:** Play with parameters of the linear layer to see how it changes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a neural network model\n",
    "To create a neural network model in Pytorch we need to define its architecture in a new class inherited from `torch.nn.Module`.\n",
    "To do that we need to define functions `__init__` and `forward`:\n",
    "\n",
    "1. The function `__init__` is a constructor in which we define the layers and any parameters we need. We must always define the function `super` inside it to initialize the parent class.\n",
    "2. The function `forward` defines the forward pass, which calculates the output $\\hat{y}$ from the input features $x_j$.\n",
    "\n",
    "Note, that we do not need to define the backward pass, this will be automatically created by Pytorch.\n",
    "\n",
    "The network architecture for a **univariate linear regression** model will consist of a **single linear layer** with $D=1$ input features and $1$ output. Our new single artificial neuron regressor `ANRegressor` is defined below: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ANRegressor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ANRegressor, self).__init__()\n",
    "        self.layer = nn.Linear(1, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer(x)      \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to create an instance of this model which we will call `net`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = ANRegressor()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that we have one linear layer with 1 input and 1 output feature, and the bias is set to `True`. We therefore obtained the univariate linear model $$y=wx+b$$\n",
    "We can also print out the learnable parameters of this model `net.parameters()` - we see that there are two:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for parameter in net.parameters():\n",
    "    print(parameter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Activity 5:** Change the number of input features in the `ANRegressor`. How many parameters to optimise you get now? Make sure that you change it back to 1 before you continue with the tutorial.\n",
    "\n",
    "**Answer:** Number of input features + 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The loss and the optimiser\n",
    "\n",
    "To fit this model to the training data we need to create the **loss** function `loss_function`. Because we are creating a regression we will use **mean squared error** (MSE)\n",
    "$$L(\\mathbf{y},\\mathbf{\\hat{y}})=\\sum_{i=1}^N(y_i-\\hat{y_i})^2$$\n",
    "where $y$ are the true target values and $\\hat{y_i}$ are the predicted target values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We select the stochastic gradient descent optimiser with learning rate $\\eta=0.2$. Note that the learnable parameters of the network `net` are the first input argument of the optimiser. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(net.parameters(), lr=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training data\n",
    "\n",
    "We will revisit example of predicting brain volumes from the age of the baby that we have seen in Week 2 of this module. Note that after loading the data, we need to \n",
    "* reshape the data to size N x 1 where N is the number of samples and 1 is the number of input (for `X`) or output (for `y`) features.\n",
    "* convert our input and output values to the Pytorch tensors\n",
    "* Convert out data into `float` values, which are required by Pytorch functions\n",
    "Note that we can plot the Pytorch tensors just like numpy arrays, as long as they do not require gradients.\n",
    "\n",
    "If working on Google colab, you first need to upload the dataset. To do that run the code below and the upload fine `'neonatal_brain_volumes.csv'` that you downloaded from KEATS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only do this if you work on Google Colab\n",
    "# run the cell\n",
    "# then upload file 'neonatal_brain_volumes.csv'\n",
    "\n",
    "from google.colab import files\n",
    "files.upload()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cell to load, convert and plot the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# load\n",
    "data = pd.read_csv('neonatal_brain_volumes.csv').to_numpy()\n",
    "\n",
    "# standardise and reshape\n",
    "X = StandardScaler().fit_transform(data[:,0].reshape(-1,1))\n",
    "y = data[:,1].reshape(-1,1)\n",
    "\n",
    "# convert\n",
    "X = torch.from_numpy(X).float()\n",
    "y = torch.from_numpy(y).float()\n",
    "print('X: ', X.shape)\n",
    "print('y: ', y.shape)\n",
    "\n",
    "# plot\n",
    "plt.plot(X, y,'*')\n",
    "plt.xlabel('age of the baby [weeks GA]')\n",
    "plt.ylabel('brain volume [mL]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training  the neural network\n",
    "The network is trained in a number of **epochs**. Each epoch consists of the following steps \n",
    "\n",
    "1. Clear gradients\n",
    "2. Forward pass: predict outputs $\\hat{y_i}$ for current estimate of network parameters $w_j^{(n)}$\n",
    "3. Compute loss $L(\\mathbf{y},\\mathbf{\\hat{y}})$ between predicted and true outputs\n",
    "4. Backward pass: calculate gradients (derivatives) $\\frac{\\partial L(\\mathbf{w}^{(n)})}{\\partial w_j^{(n)}} $ of the loss with respect to the network parameters\n",
    "5. Update the network parameters $w_j^{(n+1)}=w_j^{(n)}-\\eta \\frac{\\partial L(\\mathbf{w}^{(n)})}{\\partial w_j^{(n)}}   $\n",
    "\n",
    "During training we iterate through our data. Each iteration $n$ is called an **epoch**. During each epoch we run these 5 steps. The optimal number of the epochs depends on the dataset, the task and the learning rate. \n",
    "\n",
    "*Note: if you want to rerun this cell, you need to also rerun the cells above starting from the creating the network, because it remembers the fitted parameters.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "for i in range(epochs):\n",
    "    \n",
    "    # 1. Clear gradients \n",
    "    optimizer.zero_grad() \n",
    "    # 2. Forward pass\n",
    "    prediction = net(X) \n",
    "    # 3. Compute loss\n",
    "    loss = loss_function(prediction, y) \n",
    "    # 4. Calculate gradients\n",
    "    loss.backward()       \n",
    "    # 5. Update network parameters\n",
    "    optimizer.step() \n",
    "\n",
    "    # Display results\n",
    "    if i % 2 == 0:\n",
    "        # note how we need to tranform data back to numpy\n",
    "        #plt.cla()\n",
    "        plt.plot(X.data.numpy(), y.data.numpy(),'*')\n",
    "        plt.plot(X.data.numpy(), prediction.data.numpy(), 'r-', lw=2)\n",
    "        plt.xlabel('x')\n",
    "        plt.ylabel('f(x)')\n",
    "        plt.title(f\"Epoch={i} | Loss={loss.data.numpy():.4f}\")\n",
    "        plt.pause(0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Neural network classifier\n",
    "\n",
    "In this exercise we will implement a simple binary classifier in Pytorch to predict heart failure from the cardiac indices EF and GLS. \n",
    "\n",
    "Run the cell below to import libraries and plotting functions. Note that the plotting functions are similar to the ones used in Week 4, but `PlotClassification` had to be amended to predict data using the Pytorch rather than Scikit-learn model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "# plotting functions\n",
    "def PlotData(X,y):\n",
    "    y=y.flatten()\n",
    "    plt.plot(X[y==0,0],X[y==0,1],'bo',alpha=0.75,markeredgecolor='k',label = 'Healthy')\n",
    "    plt.plot(X[y==1,0],X[y==1,1],'rd',alpha=0.75,markeredgecolor='k',label = 'HF')\n",
    "    plt.title('Diagnosis of Heart Failure')\n",
    "    plt.xlabel('EF')\n",
    "    plt.ylabel('GLS')\n",
    "    plt.legend()\n",
    "    \n",
    "def PlotClassification(net,X,y):\n",
    "\n",
    "    # Create an 1D array of samples for each feature\n",
    "    x1 = np.linspace(-2.5, 2, 1000) \n",
    "    x2 = np.linspace(-3, 3.5, 1000).T # note the transpose\n",
    "    # Creates 2D arrays that hold the coordinates in 2D feature space\n",
    "    x1, x2 = np.meshgrid(x1, x2) \n",
    "    # Flatten x1 and x2 to 1D vector and concatenate into a feature matrix\n",
    "    Feature_space = np.c_[x1.ravel(), x2.ravel()] \n",
    "    \n",
    "    # NEW: convert numpy to torch\n",
    "    Feature_space = torch.from_numpy(Feature_space).float()\n",
    "    # NEW: Predict output scores for the whole feature space    \n",
    "    output_scores = net(Feature_space)\n",
    "    # NEW: Threshold the output probabilites\n",
    "    y_pred = output_scores>0.5\n",
    "    # NEW: Convert to numpy\n",
    "    y_pred = y_pred.numpy()\n",
    "    \n",
    "    # Resahpe to 2D\n",
    "    y_pred = y_pred.reshape(x1.shape)\n",
    "    # Plot using contourf\n",
    "    plt.contourf(x1, x2, y_pred, cmap = 'summer')\n",
    "    \n",
    "    # Plot data\n",
    "    PlotData(X,y)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training data\n",
    "\n",
    "The code below loads, and plots the training data. \n",
    "\n",
    "**Task 1.1:** Complete the code to convert the feature matrix `X` and labels `y` to Pytorch tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only do this if you work on Google Colab\n",
    "# run the cell\n",
    "# then upload file 'heart_failure_data.csv'\n",
    "\n",
    "from google.colab import files\n",
    "files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load, standardise and reshape the training data\n",
    "df = pd.read_csv('heart_failure_data.csv')\n",
    "scaler = StandardScaler()\n",
    "data = df.to_numpy()\n",
    "X = scaler.fit_transform(data[:,:2])\n",
    "y = data[:,2].reshape(-1,1)\n",
    "\n",
    "# convert to tensors\n",
    "\n",
    "\n",
    "print('X: ', X.shape)\n",
    "print('y: ', y.shape)\n",
    "\n",
    "# Plot data\n",
    "PlotData(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network architecture\n",
    "\n",
    "Our simple binary linear classifier network will be in fact **logistic regression**.\n",
    "\n",
    "To create the network architecture we will need a single **linear layer** $z=\\sum_{j=0}^Dw_jx_j$ with $D=2$ input features (EF, GLS) and one output feature (HF):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.Linear(2,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we are building a logistic regression classifier, we choose a **sigmoid** $\\sigma(z)=\\frac{1}{1+e^{-z}}$ as an activation function $f(z)$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.Sigmoid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss function will be **binary cross-entropy** \n",
    "$$L(\\mathbf{y},\\mathbf{\\hat{p}})=-\\sum_{i=1}^N(y_i\\log\\hat{p_i}+(1-y_i)\\log(1-\\hat{p_i}))$$\n",
    "where $\\hat{p_i}=\\sigma(z)$. The binary cross-entropy loss in Pytorch is "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.BCELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 1.2:** Complete the code to create a linear binary classifier network using the building blocks shown above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define network architecture\n",
    "class ANClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ANClassifier, self).__init__()\n",
    "        self.layer = None\n",
    "        self.sigmoid = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "**Task 1.3:** Fill in the code to create the network instance, loss and optimiser, as well as the 5 steps of training that are performed during each epoch. The code the plot the fitted model is provided for you. Some code has been given but commented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create network \n",
    "net2 = None\n",
    "\n",
    "# Loss\n",
    "loss_function = None\n",
    "\n",
    "# Optimizer\n",
    "optimizer = None #torch.optim.SGD(net2.parameters(), lr=0.2)\n",
    "\n",
    "# Training\n",
    "epochs = 100\n",
    "for i in range(epochs):\n",
    "    \n",
    "    # 1. Clear gradients \n",
    "    \n",
    "    # 2. Forward pass\n",
    "    prediction = None\n",
    "    # 3. Compute loss\n",
    "    loss = None\n",
    "    # 4. Calculate gradients\n",
    "           \n",
    "    # 5. Update network parameters\n",
    "     \n",
    "    \n",
    "# Plot classification result\n",
    "#PlotClassification(net2,X,y) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
