{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Non-Linear Manifold Learning\n",
    "\n",
    "In the last class we explored some linear manifold learning techniques, such as PCA, and saw how they can be applied to problems in Biomedical Imaging.\n",
    "\n",
    "In this class we will look at **non-linear manifold learning**. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The motivation for our interest in manifold learning was that, in many biomedical applications, our datasets have a **very high dimensionality** which makes analysis challenging. \n",
    "\n",
    "However, the data will usually be generated by some **natural process with fewer degress of freedom** than the dimensionality of the data would suggest. \n",
    "\n",
    "What we want are methods to extract these underlying degrees of freedom in the data.\n",
    "\n",
    "To understand this better, let's look at a simple example in which linear methods like PCA fail to uncover the data's natural **non-linear structure**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\" Non-Linear Manifold Learning notebook\n",
    "\n",
    "James Clough 2018-2019\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D # for 3D plotting with matplotlib\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.decomposition import PCA   # sklearn has a nice PCA implementation\n",
    "from scipy.linalg import eigh           # eigendecomposition\n",
    "\n",
    "# display plots in the notebook\n",
    "%matplotlib inline                      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "First, we will generate a swiss-roll dataset, like the one we saw in the previous class.\n",
    "\n",
    "We start with a set of $N$ points scattered in a 2D unit square, and will then map those points to lie on a spiral shape in 3D.\n",
    "\n",
    "If you like, you can try changing the variables here to generate different datasets.\n",
    "\n",
    "The variable $N$ sets the number of datapoints in the swiss-roll. \n",
    "\n",
    "num_rotations sets the number of full rotations of the swiss-roll spiral."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\" Let's generate a dataset in which linear ML methods will fail to uncover the data's natural structure \"\"\"\n",
    "\n",
    "N = 2000                       # number of datapoints\n",
    "num_rotations = 1.2\n",
    "X_m = np.random.random((N, 2)) # random points scattered in [0,1]^2\n",
    "\n",
    "def create_spiral(X, num_rotations):\n",
    "    \"\"\" Take 2D manifold X and output 3D spiral made from curling up X in 3D space \"\"\"\n",
    "    N = X.shape[0]\n",
    "    r = np.exp(X[:,1] * num_rotations) * 0.5\n",
    "    theta = X[:,1] * (2 * np.pi) * num_rotations\n",
    "    Y = np.zeros((N, 3))\n",
    "    Y[:,0] = X[:,0] * 6\n",
    "    Y[:,1] = r * np.cos(theta)\n",
    "    Y[:,2] = r * np.sin(theta)\n",
    "    return Y\n",
    "\n",
    "Y = create_spiral(X_m, num_rotations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now we have generated the swiss-roll dataset, we can plot it.\n",
    "\n",
    "Think of the original 2D data as the 'underlying degrees of freedom'.\n",
    "\n",
    "Think of the 3D data as the high-dimensional data we have measured."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\" Plot the manifold curled up into a 3D swiss-roll\"\"\"\n",
    "\n",
    "fig = plt.figure(figsize=(16,8))\n",
    "ax = fig.add_subplot(121)\n",
    "ax.set_xticks([])\n",
    "ax.set_yticks([])\n",
    "_ = ax.scatter(X_m[:,0], X_m[:,1], c=X_m[:,1], marker='o')\n",
    "\n",
    "ax = fig.add_subplot(122, projection='3d')\n",
    "ax.set_xticks([])\n",
    "ax.set_yticks([])\n",
    "ax.set_zticks([])\n",
    "_ = ax.scatter(Y[:,0], Y[:,1], Y[:,2], c=X_m[:,1], marker='o')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Principal Component Analysis\n",
    "\n",
    "As we saw last week, PCA projects the data onto the hyperplane minimising the approximation error, or, equivalently, maxmising the variance along the hyperplane.\n",
    "\n",
    "\n",
    "### What do you expect to happen if we use PCA to reduce the dimensionality of this swiss-roll dataset from 3 dimensions to 2?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\" Try using PCA on this dataset and see if it can recover the underlying 2D structure \"\"\"\n",
    "\n",
    "# In scikit-learn, we first create a PCA object...\n",
    "# n_components refers to the number of principal components we're extracting\n",
    "# ie. the dimensionality of the space we are trying to find\n",
    "pca = PCA(n_components=2)\n",
    "\n",
    "# Then apply in to our dataset\n",
    "X_pca = pca.fit_transform(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,6))\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "_ = plt.scatter(X_pca[:,0], X_pca[:,1], c=X_m[:,1], marker='o')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Was this what you expected? \n",
    "\n",
    "### Can you explain what's going on here?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The exact shape of the data you see will vary depending on the random points you started with, but it is unlikely that you will recover something very much like our original 2D dataset, and instead the spiral structure we are trying to unfold will still be there.\n",
    "\n",
    "We have failed to uncover the 2D manifold structure because PCA can only project the data on to a hyperplane\n",
    "and it can't unfurl curved manifolds. \n",
    "\n",
    "Another way of thinking about this problem is to notice that we want to measure the distances between points as the distance *along the manifold* (ie. along the swiss-roll) but if the manifold is curved, that is not the same as the distance between the points in the high-dimensional space (ie. the distance in 3D space not confined to the surface of the swiss-roll)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Laplacian Eigenmaps\n",
    "\n",
    "To try and solve this problem we need non-linear manifold learning.\n",
    "\n",
    "There are lots of non-linear manifold learning algorithms, but we'll be using one called Laplacian Eigenmaps, which is a simple but effective and commonly used algorithm.\n",
    "\n",
    "**Laplacian Eigenmaps for Dimensionality Reduction and Data Representation**, *Belkin and Niyogi, Neural Computation, 2003*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The key idea is this: even if the manifold that the data lies on is curved, we can assume that it is ***locally flat***. \n",
    "\n",
    "This means that the distances between points in the original data are still meaningful as long as those points are close to each other. \n",
    "\n",
    "If we use only the nearest neighbours of each point in the embedding we can get around this problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Nearest neighbours\n",
    "The first step in many manifold learning methods is to find the k-nearest neighbours of each point. \n",
    "\n",
    "This just means that we choose a number, $k$, and for each point in our data, find the $k$ points which are nearest to it.\n",
    "\n",
    "The result can be described mathematically as a ***graph***. \n",
    "\n",
    "Here, a graph is not the thing with an x-axis and a y-axis and a line - rather it is a mathematical object (studied in Graph Theory - https://en.wikipedia.org/wiki/Graph_theory) that consists of some points, or **vertices**, and some relations between them, or **edges**.\n",
    "\n",
    "In this case, the vertices in the graph are our datapoints, and there will be an edge between two points if one of them is in the k-nearest neighours of the other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"imgs/scatter.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"imgs/scatter_knn.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's write a function that finds the k-nearest neighbours of each point in our dataset.\n",
    "\n",
    "## Exercise 1\n",
    "\n",
    "Write a function, ***my_knn*** that takes in an array of size NxD of the datapoints, and an integer k, and returns an NxN array where the [i,j] element of the array is a 1 if j is a k-nearest-neighbour of i (excluding i itself), and 0 otherwise.\n",
    "\n",
    "Don't worry too much about doing this in a clever or fast way, just make sure your function gives you the correct answer.\n",
    "\n",
    "*Hint - numpy has a function called **argsort** that you might find useful*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def my_knn(X, k):\n",
    "    \"\"\" Finds k-nearest neighbours in X\"\"\"\n",
    "    N, D = X.shape\n",
    "    A = np.zeros((N,N))\n",
    "    ###\n",
    "    ###\n",
    "    ###\n",
    "    ###\n",
    "    return A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def my_knn(X, k):\n",
    "    \"\"\" Finds k-nearest neighbours in X \"\"\"\n",
    "    N, D = X.shape\n",
    "    A = np.zeros((N, N))\n",
    "    for i in range(N):\n",
    "        i_sq_distances = np.sum((X - X[i])**2, axis=1)\n",
    "        nearest_points = np.argsort(i_sq_distances)\n",
    "        # [1:k+1] is because the nearest point to i is i itself - but we don't want it\n",
    "        k_nearest      = nearest_points[1:k+1]\n",
    "        for j in k_nearest:\n",
    "            A[i,j] = 1\n",
    "    return A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "A = my_knn(Y, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Nearest neighbours check\n",
    "Lets check our function is doing what we expect it to by plotting a point with its nearest neighbours highlighted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "i = 2\n",
    "knn_i = A[i].nonzero()\n",
    "\n",
    "fig = plt.figure(figsize=(16,8))\n",
    "\n",
    "ax = fig.add_subplot(121, projection='3d')\n",
    "ax.set_xticks([])\n",
    "ax.set_yticks([])\n",
    "ax.set_zticks([])\n",
    "_ = ax.scatter(Y[:,0], Y[:,1], Y[:,2], c=X_m[:,1], marker='o')\n",
    "_ = ax.scatter(Y[i,0], Y[i,1], Y[i,2], c='r', marker='o', s=300) # plot point i\n",
    "_ = ax.scatter(Y[knn_i,0], Y[knn_i,1], Y[knn_i,2], c='b', marker='o', s=100) # plot i's nearest neighbours\n",
    "\n",
    "\n",
    "ax = fig.add_subplot(122)\n",
    "ax.set_xticks([])\n",
    "ax.set_yticks([])\n",
    "_ = ax.scatter(X_m[:,0], X_m[:,1], c=X_m[:,1], marker='o')\n",
    "_ = ax.scatter(X_m[knn_i,0], X_m[knn_i,1], c='b', marker='o', s=100) # plot i's nearest neighbours\n",
    "_ = ax.scatter(X_m[i,0], X_m[i,1], c='r', marker='o', s=300) # plot point i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Laplacian Eigenmaps\n",
    "\n",
    "Our aim is to find low-dimensional coordinates which maintain the near-neighbour relationships from the original data.\n",
    "\n",
    "We will say that we are trying to minimise the following cost function:\n",
    "\n",
    "$C = \\sum_{i,j} (x_i - x_j)^2 \\mathbf{A}_{ij}$\n",
    "\n",
    "Where $\\mathbf{A}_{ij}$ is our nearest-neighbour matrix which tells us whether two points are close together in the original dataset, and the $x_i$ and $x_j$ represent the low-dimensional coordinates we're trying to find.\n",
    "\n",
    "$\\mathbf{A}$ is fixed, and determined by our input data.\n",
    "$x$ is the solution we are trying to find."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "$C = \\sum_{i,j} (x_i - x_j)^2$  <span style=\"color:red\"> $\\mathbf{A}_{ij} $</span>\n",
    "\n",
    "You can hopefully see that this <span style=\"color:red\"> </span> cost function is minimised if we place points which are close together in the original data (ie. $\\mathbf{A}_{ij}$ is high), close together in the new low-dimensional representation (so that $(x_i - x_j)^2$ is low).\n",
    "\n",
    "Now we know what we are trying to minimise we can mathematically describe our algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "$C = \\sum_{i,j}$ <span style=\"color:red\"> $(x_i - x_j)^2 $</span> $\\mathbf{A}_{ij}$\n",
    "\n",
    "You can hopefully see that this cost function is minimised if we place points which are close together in the original data (ie. $\\mathbf{A}_{ij}$ is high), close together in the new low-dimensional representation (so that $(x_i - x_j)^2$ is low).\n",
    "\n",
    "Now we know what we are trying to minimise we can mathematically describe our algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "$C = \\sum_{i,j} (x_i - x_j)^2 \\mathbf{A}_{ij}$\n",
    "\n",
    "Question: what is the problem with this cost function?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "$C = \\sum_{i,j} (x_i - x_j)^2 \\mathbf{A}_{ij}$\n",
    "\n",
    "Question: what is the problem with this cost function?\n",
    "\n",
    "Hint: It has a trivial solution we do not want"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "$C = \\sum_{i,j} (x_i - x_j)^2 \\mathbf{A}_{ij}$\n",
    "\n",
    "Question: what is the problem with this cost function?\n",
    "\n",
    "Hint: It has a trivial solution we do not want\n",
    "\n",
    "Solution: $C$ is minimised by setting all of the coordinates to be equal: $x_i=x_j$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Therefore we also require that the points $x$ are spaced out from each other - ie.\n",
    "\n",
    "$\\mathbf{x}^T \\mathbf{D} \\mathbf{x} = 1$\n",
    "\n",
    "where $\\mathbf{D}$ is a diagonal matrix given by $\\mathbf{D}_{ii} = \\sum_j \\mathbf{A}_{ij}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "$C = \\sum_{i,j} (x_i - x_j)^2 \\mathbf{A}_{ij}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "$C = \\sum_{i,j} (x_i - x_j)^2 \\mathbf{A}_{ij}$\n",
    "\n",
    "$C = \\sum_{i,j} (x_i^2 + x_j^2 - 2 x_i x_j) \\mathbf{A}_{ij}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "$C = \\sum_{i,j} (x_i - x_j)^2 \\mathbf{A}_{ij}$\n",
    "\n",
    "$C = \\sum_{i,j} (x_i^2 + x_j^2 - 2 x_i x_j) \\mathbf{A}_{ij}$\n",
    "\n",
    "$C = \\sum_i x_i^2 \\mathbf{D_{ii}} +  \\sum_j x_j^2 \\mathbf{D_{jj}} - 2 \\sum_{i,j} x_i x_j \\mathbf{A}_{ij}$\n",
    "\n",
    "where $\\mathbf{D}$ is a diagonal matrix given by $\\mathbf{D}_{ii} = \\sum_j \\mathbf{A}_{ij}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "$C = \\sum_{i,j} (x_i - x_j)^2 \\mathbf{A}_{ij}$\n",
    "\n",
    "$C = \\sum_{i,j} (x_i^2 + x_j^2 - 2 x_i x_j) \\mathbf{A}_{ij}$\n",
    "\n",
    "$C = \\sum_i x_i^2 \\mathbf{D_{ii}} + \\sum_j x_j^2 \\mathbf{D_{jj}} - 2 \\sum_{i,j} x_i x_j \\mathbf{A}_{ij}$\n",
    "\n",
    "$C = 2 \\sum_i x_i^2 \\mathbf{D_{ii}} - 2 \\sum_{i,j} x_i x_j \\mathbf{A}_{ij}$\n",
    "\n",
    "$C = 2 \\mathbf{x}^T \\mathbf{L} \\mathbf{x}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We can find\n",
    "\n",
    "$\\min \\mathbf{x}^T \\mathbf{L} \\mathbf{x}$ \n",
    "\n",
    "subject to the constraint\n",
    "\n",
    "$\\mathbf{x}^T \\mathbf{D} \\mathbf{x} = 1$ \n",
    "\n",
    "by finding the eigenvectors of the matrix $\\mathbf{L}$\n",
    "\n",
    "\n",
    "This matrix is called the **Laplacian** and has many useful properties."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Graph Laplacian\n",
    "Graphs can be represented in many ways, usually by matrices. The most common representation is an **adjacency matrix**, $\\mathbf{A}$ which is what our ***my_knn*** function above generates.\n",
    "\n",
    "Another matrix representation is the Laplacian matrix, $L$, which is calculated by:\n",
    "\n",
    "$\\mathbf{L} = \\mathbf{D} - \\mathbf{A}$\n",
    "\n",
    "where $\\mathbf{D}$ is a diagonal matrix given by $\\mathbf{D}_{ii} = \\sum_j \\mathbf{A}_{ij}$.\n",
    "\n",
    "The Laplacian matrix can be thought of as a discrete version of the *Laplace operator* which describes diffusion on a surface. \n",
    "\n",
    "By looking at the eigenvectors of this matrix (equivalent to the eigenmodes of the Laplace operator) we can see the most significant modes of variation in the graph representing our original data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The result of your k-nearest-neighbours function should be an NxN matrix, which has a 1 for nearest-neighbours and a 0 otherwise. \n",
    "\n",
    "To use in the Laplacian Eigenmaps method, this matrix needs to be symmetric. \n",
    "\n",
    "Does your knn algorithm always return a symmetric matrix? If not, we can symmetrise it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def symmetrise(X):\n",
    "    \"\"\" Symmetrises the matrix X.\n",
    "    \n",
    "    Notes\n",
    "    -----\n",
    "    np.maximum returns the element-wise maximum of its arguments.\"\"\"\n",
    "    return np.maximum(X, X.T)\n",
    "A = symmetrise(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# plot first few eigenmodes of this swiss-roll dataset\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "\n",
    "D = np.diag(np.sum(A, axis=0))\n",
    "L = D - A\n",
    "v, C = eigh(L, eigvals=(1,6))\n",
    "\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.set_xticks([])\n",
    "ax.set_yticks([])\n",
    "ax.set_zticks([])\n",
    "_ = ax.scatter(Y[:,0], Y[:,1], Y[:,2], c=C[:,0], marker='o')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# plot first few eigenmodes of this swiss-roll dataset\n",
    "fig = plt.figure(figsize=(16,8))\n",
    "\n",
    "D = np.diag(np.sum(A, axis=0))\n",
    "L = D - A\n",
    "v, C = eigh(L, eigvals=(1,6))\n",
    "\n",
    "ax1 = fig.add_subplot(121, projection='3d')\n",
    "ax1.set_xticks([])\n",
    "ax1.set_yticks([])\n",
    "ax1.set_zticks([])\n",
    "_ = ax1.scatter(Y[:,0], Y[:,1], Y[:,2], c=C[:,0], marker='o')\n",
    "\n",
    "ax2 = fig.add_subplot(122, projection='3d')\n",
    "ax2.set_xticks([])\n",
    "ax2.set_yticks([])\n",
    "ax2.set_zticks([])\n",
    "_ = ax2.scatter(Y[:,0], Y[:,1], Y[:,2], c=C[:,1], marker='o')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# plot first few eigenmodes of this swiss-roll dataset\n",
    "fig = plt.figure(figsize=(24,8))\n",
    "\n",
    "D = np.diag(np.sum(A, axis=0))\n",
    "L = D - A\n",
    "v, C = eigh(L, eigvals=(1,6))\n",
    "\n",
    "ax1 = fig.add_subplot(131, projection='3d')\n",
    "ax1.set_xticks([])\n",
    "ax1.set_yticks([])\n",
    "ax1.set_zticks([])\n",
    "_ = ax1.scatter(Y[:,0], Y[:,1], Y[:,2], c=C[:,0], marker='o')\n",
    "\n",
    "ax2 = fig.add_subplot(132, projection='3d')\n",
    "ax2.set_xticks([])\n",
    "ax2.set_yticks([])\n",
    "ax2.set_zticks([])\n",
    "_ = ax2.scatter(Y[:,0], Y[:,1], Y[:,2], c=C[:,1], marker='o')\n",
    "\n",
    "ax3 = fig.add_subplot(133, projection='3d')\n",
    "ax3.set_xticks([])\n",
    "ax3.set_yticks([])\n",
    "ax3.set_zticks([])\n",
    "_ = ax3.scatter(Y[:,0], Y[:,1], Y[:,2], c=C[:,2], marker='o')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now we can implement the Laplacian Eigenmaps method.\n",
    "\n",
    "1) Find the k-nearest-neighbours matrix of your data - call this A\n",
    "\n",
    "2) Find the Laplacian of A - call this L\n",
    "\n",
    "3) Find the eigenvectors corresponding to the d smallest non-zero eigenvalues of L\n",
    "\n",
    "These d eigenvectors are the coordinates of each of your points in a d-dimensional representation of your data.\n",
    "For now we can choose d=2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Exercise 2\n",
    "Write a function that takes in an adjacency matrix and implements Laplacian eigenmap method described above.\n",
    "\n",
    "*Hint - scipy has a function called scipy.linalg.eigh which can help you fund the eigenvectors of a matrix efficiently*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def my_laplacian_eigenmap(Y, k=20, d=2):\n",
    "    A = my_knn(Y, k)\n",
    "    ###\n",
    "    ###\n",
    "    ###\n",
    "    ###\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def my_laplacian_eigenmap(Y, k=20, d=2):\n",
    "    A = my_knn(Y, k)\n",
    "    A = symmetrise(A)\n",
    "    D = np.diag(np.sum(A, axis=0))\n",
    "    L = D - A\n",
    "    v, X = eigh(L, eigvals=(1,d))\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "Z = my_laplacian_eigenmap(Y)\n",
    "\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.set_xticks([])\n",
    "ax.set_yticks([])\n",
    "_ = ax.scatter(Z[:,0], Z[:,1], c=X_m[:,1], marker='o')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Can your method recover the original manifold structure? Try changing the the number of nearest neighbours $k$. What happens when $k$ is set very high, or very low?"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
